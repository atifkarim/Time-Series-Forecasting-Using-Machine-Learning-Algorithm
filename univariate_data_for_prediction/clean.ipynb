{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "from numpy import nan\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import collections\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to read the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(filepath):\n",
    "    test = pd.read_csv(filepath) # here the given csv file is reading\n",
    "    return test\n",
    "\n",
    "filepath = 'E:/University of Bremen MSc/masters_thesis/IAT_sebastian/dataset_26_april_3.csv'\n",
    "initial_dataframe = create_dataframe(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this cell dateTime column will be made but will not set it up as index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting timestamp to unixtime and return the ready dataframe\n",
    "\n",
    "def conversion_timestamp_to_unixtime(initial_dataframe):\n",
    "    ''' now conversion of timestamp to unixtime will start. In the csv file the column name of\n",
    "    timestamp is longtime.'''\n",
    "    \n",
    "    longTime = initial_dataframe.loc[0:,['longTime']]\n",
    "    longTime = longTime.as_matrix()\n",
    "    a = []\n",
    "    date_time_array = []\n",
    "    for k in longTime:\n",
    "        a = np.append(a,k)\n",
    "    str_time = []\n",
    "    correct_longtime = []\n",
    "    datetime_time = []\n",
    "    count = 0\n",
    "    \n",
    "    for b in a:\n",
    "        b = int(b) # make plain integer\n",
    "        str_b = str(b)\n",
    "        c = str_b[-3:]\n",
    "        new_str_b = str_b.replace(c, '',1)\n",
    "        new_str_b_time = int(new_str_b)\n",
    "        correct_longtime.append(new_str_b_time)\n",
    "        now_time = datetime.datetime.fromtimestamp(new_str_b_time)\n",
    "        convert_time = now_time.strftime('%Y-%m-%d %H:%M')\n",
    "        str_time.append(convert_time)\n",
    "    test_new = initial_dataframe.assign(stringTime=str_time,correct_longtime=correct_longtime) # here new column in the panda dataframe for string_time has added\n",
    "    test_new['dateTime'] =  pd.to_datetime(test_new['stringTime'], format='%Y-%m-%d %H:%M')\n",
    "    test_new = test_new.drop(['longTime','stringTime','correct_longtime'], axis=1)\n",
    "    \n",
    "    return test_new\n",
    "\n",
    "test_new = conversion_timestamp_to_unixtime(initial_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look here in the previous cell if test.head() is printed then 'row ID' column will be appeared which is problematic. so, remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_1 = test_new.drop(['row ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_new.shape)\n",
    "print(test_new_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test_new.reset_index()\n",
    "b = test_new_1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# be careful here , when perform on a dataframe reset_index then a new column will appear and it is 'index'. No need of it so immediately drop it. for better view please take a look in the previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it's time to alter the dataframe. This will oreder the dataframe in ascending prder with respect to dateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here using test_new_1 as it has no row ID column and also no problematic value\n",
    "\n",
    "def alter_time(dataframe, start_pos, end_pos):\n",
    "#     multivariate_data=test_new.iloc[start_pos:end_pos][multivariate_column_label] # comment out this line if you pass column label\n",
    "    dataframe=dataframe.iloc[start_pos:end_pos][:]\n",
    "    dataframe=dataframe.loc[::-1]\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "start_pos = 0\n",
    "end_pos = 25000\n",
    "multivariate_data = alter_time(test_new_1, start_pos, end_pos)\n",
    "multivariate_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now target column and dateTime colum will be arranged as a given column index. Here target column is the output of turbine 9's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_frame(dataframe,colname,col_pos):\n",
    "    \n",
    "    list_col=dataframe.columns.to_list()\n",
    "    temp_list=list_col\n",
    "    for idx,i in enumerate(colname):\n",
    "        sacrifice_val = temp_list[col_pos[idx]]        \n",
    "        indx=dataframe.columns.get_loc(i)\n",
    "        temp_list[col_pos[idx]]=i\n",
    "        temp_list[indx]=sacrifice_val\n",
    "        \n",
    "    return dataframe.iloc[:][temp_list]\n",
    "index_array=[0,-1]\n",
    "req_column_name = ['dateTime','AEWIHO_T9AV2']\n",
    "rearranged_dataframe = rearrange_frame(multivariate_data,req_column_name,index_array)\n",
    "rearranged_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multivariate_data.shape)\n",
    "print(rearranged_dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now take in consideration the signal DEWIHOBT9_I0. When the value of it's will be 100 only then target column will work otherwise not. So, choose this signal and drop all of the rows where it's value != 100 and then drop the whole colum as after dropping this column will only contain value 100 and it will affect negatively in the correlation with target signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function will do the following task\n",
    "# if the blast furnace signal for turbine 9 is zero then no work will be happened.\n",
    "# so, remove all the rows where this value will be zero\n",
    "\n",
    "def drop_zero_value_row_of_blast_furnace_signal(dataframe, blast_furnace_signal):\n",
    "#     dataframe = dataframe.reset_index()\n",
    "    count = []\n",
    "    print(blast_furnace_signal)\n",
    "    for idx_blast_furnace, val_blast_furnace in enumerate(dataframe[blast_furnace_signal]):\n",
    "        if val_blast_furnace != 100 :\n",
    "            count = np.append(count,idx_blast_furnace)\n",
    "    print('size of count array here: ', count.size)\n",
    "    \n",
    "    if count.size > 0:\n",
    "        dataframe_1 = dataframe.drop(count,axis=0) # axis= 0 means row indiated. 1 means column indicated\n",
    "    else:\n",
    "        dataframe_1 = dataframe\n",
    "    dataframe_1 = dataframe_1.drop([blast_furnace_signal], axis=1) # dropping the column. because all value are same   \n",
    "    return dataframe_1\n",
    "blast_furnace_signal = 'DEWIHOBT9_I0'\n",
    "\n",
    "dataframe_no_zero_value_blast_furnace = drop_zero_value_row_of_blast_furnace_signal(rearranged_dataframe,blast_furnace_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_no_zero_value_blast_furnace.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(rearranged_dataframe.shape)\n",
    "print(dataframe_no_zero_value_blast_furnace.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now choose the target colum  and check if any value is zero or not. If zero then drop those rows. here taret column is T9's output, signal name is AEWIHO_T9AV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_zero_value_row_of_target_signal(dataframe, target_signal):\n",
    "    count = []\n",
    "    for idx_blast_furnace, val_blast_furnace in enumerate(dataframe[target_signal]):\n",
    "        if val_blast_furnace == 0 :\n",
    "            count = np.append(count,idx_blast_furnace)\n",
    "\n",
    "    for i in count:\n",
    "        if i > 24222:\n",
    "            print(i)\n",
    "    print('size of count array: ', count.size)\n",
    "\n",
    "    if count.size > 0:\n",
    "        dataframe_1 = dataframe.drop(count,axis=0) # axis= 0 means row indiated. 1 means column indicated\n",
    "    else:\n",
    "        dataframe_1 = dataframe\n",
    "    dataframe_1 = dataframe_1.drop(dataframe_1.columns[0], axis = 1) # generally after resetting index the former index \n",
    "                                                                                            # take place the first place of the column. so removing it.\n",
    "    return dataframe_1\n",
    "\n",
    "\n",
    "target_signal = 'AEWIHO_T9AV2'\n",
    "dataframe_reset = dataframe_no_zero_value_blast_furnace.reset_index()\n",
    "dataframe_no_zero_value_target_column = drop_zero_value_row_of_target_signal(dataframe_reset,target_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_no_zero_value_target_column.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataframe_no_zero_value_blast_furnace.shape)\n",
    "print(dataframe_no_zero_value_target_column.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now drop all columns whose all vaues are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_column_with_same_value(dataframe):\n",
    "    cols = dataframe.select_dtypes([np.number]).columns\n",
    "    diff = dataframe[cols].diff().sum()\n",
    "    dataframe_drop_column_with_same_value = dataframe.drop(diff[diff== 0].index, axis=1)\n",
    "    \n",
    "    return dataframe_drop_column_with_same_value\n",
    "\n",
    "dataframe_drop_column_with_same_value = drop_column_with_same_value(dataframe_no_zero_value_target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_drop_column_with_same_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe_no_zero_value_target_column.shape)\n",
    "print(dataframe_drop_column_with_same_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check on the whole dataframe if there is any NAN value or not. If YES, replace it with zero and drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think twice before using this function\n",
    "# checking if any column has nan value or not. If YES then replace nan with zero and drop the row\n",
    "\n",
    "# a = dataframe_no_zero_value_blast_furnace[blast_furnace_signal].isnull().sum()\n",
    "# print(a)\n",
    "\n",
    "def drop_nan_value(dataframe):\n",
    "    for index,column in enumerate(dataframe):\n",
    "        nan_catcher = dataframe[column].isnull().sum()\n",
    "        if nan_catcher !=0:\n",
    "            dataframe_1 = dataframe[column].replace(0,nan)\n",
    "            dataframe_1 = dataframe.dropna(how='any',axis=0)\n",
    "#             print(column,' has total',nan_catcher, 'nan valu')\n",
    "        else:\n",
    "            dataframe_1 = dataframe\n",
    "#             print(column,' is free from nan value. look it has: ', nan_catcher,' value')\n",
    "            \n",
    "    return dataframe_1\n",
    "\n",
    "multivariate_data_drop_nan = drop_nan_value(dataframe_drop_column_with_same_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_data_drop_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def drop_row(dataframe):\n",
    "    \n",
    "    for i in dataframe:\n",
    "        print(i)\n",
    "        dataframe_drop_row_consecutive_same_value = dataframe.loc[dataframe[i].shift() != dataframe[i]]\n",
    "    \n",
    "    return dataframe_drop_row_consecutive_same_value\n",
    "\n",
    "dataframe_drop_row_consecutive_same_value = drop_row(multivariate_data_drop_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_string_column(dataframe):\n",
    "    drop_object = dataframe.select_dtypes(exclude=['object'])\n",
    "    \n",
    "    return drop_object\n",
    "\n",
    "dataframe_no_string = drop_string_column(dataframe_drop_row_consecutive_same_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multivariate_data_drop_nan.shape)\n",
    "print(dataframe_drop_row_consecutive_same_value.shape)\n",
    "print(dataframe_no_string.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data cleaning process has done. Now feature selection process will come. Before doing this just make a copy of dataframe and set the index as dateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_copy = dataframe_no_string.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_datetime = dataframe_copy.set_index('dateTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe_copy.shape)\n",
    "print(dataframe_datetime.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn feature selection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_with_selectKbest(dataframe,max_best_number):\n",
    "    train_input = dataframe.iloc[:,:-1]\n",
    "    train_output = dataframe.iloc[:,-1]\n",
    "    train_output = train_output.to_frame()\n",
    "#     train_output = pd.DataFrame(train_output)\n",
    "    \n",
    "    X, y = train_input, train_output\n",
    "    X = X.astype(int)\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=2)\n",
    "    fit = bestfeatures.fit(X,y)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X.columns)\n",
    "    \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "#     print(featureScores.nlargest(20,'Score'))  #print 10 best features\n",
    "    d = featureScores.nlargest(max_best_number,'Score')\n",
    "    \n",
    "    e = []\n",
    "    for i,v in enumerate(d['Specs']):\n",
    "        e = np.append(e,v)\n",
    "    \n",
    "    e = np.append(e,dataframe.columns[-1])\n",
    "    final_dataframe = dataframe.iloc[:][e]\n",
    "    \n",
    "    return final_dataframe\n",
    "max_best_number = 20\n",
    "sklearn_feature_best_dataframe = feature_selection_with_selectKbest(dataframe_datetime,max_best_number)\n",
    "sklearn_feature_best_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataframe_datetime.shape)\n",
    "print(sklearn_feature_best_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe_datetime['DEHGF_CHYPI2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection with correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find correlated matrix for dataframe which came from sklearn feature selection and the datafarem which has passed\n",
    "# to sklearn feature selection function\n",
    "\n",
    "def pearson_correlation(sklearn_dataframe, main_dataframe):\n",
    "    sklearn_correlation = sklearn_dataframe.corr()\n",
    "    main_correlation = main_dataframe.corr()\n",
    "    return sklearn_correlation, main_correlation\n",
    "\n",
    "sklearn_correlation, main_correlation = pearson_correlation(sklearn_feature_best_dataframe, dataframe_datetime)\n",
    "print(sklearn_correlation.shape)\n",
    "print(main_correlation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use the correlation matrix to make the new dataframe where the feature will be the column who has a correlation value with the target in a given range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make dataframe with high correlated valued column\n",
    "def make_dataframe_with_high_correlated_value(main_dataframe,correlated_dataframe,\n",
    "                                              correlation_threshold_value,max_value):\n",
    "    \n",
    "    target_column = main_dataframe.columns[-1]\n",
    "    \n",
    "    dataframe = correlated_dataframe.reset_index()\n",
    "    \n",
    "    high_correlated_array_with_target = []\n",
    "    for index_corr_reset, val_corr_reset in enumerate(dataframe[target_column]):\n",
    "        if val_corr_reset > correlation_threshold_value and val_corr_reset < max_value:\n",
    "            required_column = dataframe.loc[index_corr_reset]['index']\n",
    "            if required_column != target_column:\n",
    "                high_correlated_array_with_target = np.append(high_correlated_array_with_target,required_column)\n",
    "            else:\n",
    "                print(required_column)\n",
    "                pass\n",
    "            \n",
    "    final_array = np.append(high_correlated_array_with_target,target_column)\n",
    "    new_dataframe = main_dataframe.iloc[:][final_array]\n",
    "    \n",
    "    return new_dataframe\n",
    "\n",
    "correlation_threshold_value = 0.5\n",
    "max_value = 0.9\n",
    "# target_column = dataframe_datetime.columns[-1] # here declaring who is target column.\n",
    "\n",
    "\n",
    "main_frame = dataframe_datetime\n",
    "correlated_frame = main_correlation\n",
    "\n",
    "# main_frame = sklearn_feature_best_dataframe\n",
    "# correlated_frame = sklearn_correlation\n",
    "\n",
    "dataframe_high_correlation = make_dataframe_with_high_correlated_value(main_frame,correlated_frame,\n",
    "                                                             correlation_threshold_value,max_value)\n",
    "\n",
    "print('dataframe_high_correlation shape: ', dataframe_high_correlation.shape)\n",
    "print('dataframe_datetime shape: ', dataframe_datetime.shape)\n",
    "print('correlated_frame shape: ', correlated_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe_high_correlation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_datetime.iloc[0:].plot(y = dataframe_datetime.columns[-1], use_index=True)\n",
    "plt.rcParams['figure.figsize'] =(15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now dataframe for weekday, weekend, daywise, shiftwise will be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_datetime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataframe_index_datetime = dataframe_datetime.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(index = dataframe_datetime.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df2 = dataframe_datetime.loc[:,dataframe_datetime.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(target_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.concat([df2, target_df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['dateTime_column'] =  pd.to_datetime(dataframe_datetime.index, format='%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking type of the contect of the column\n",
    "s = df2['dateTime_column'].dtype\n",
    "print(s)\n",
    "\n",
    "# checking column type\n",
    "t = df2.dateTime_column\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2['Date'] = df2[df2.index].dt.strftime('%d/%m/%Y')\n",
    "# df2['Time'] = df2[df2.index].dt.strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will return a column with weekday name\n",
    "df2['Weekday_name'] = df2.index.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['TypeofDAY'] = np.where(df2['dateTime_column'].dt.dayofweek < 5, 'Weekday', 'Weekend') # if the associated number less than 5 then weekend, otherwise weekday\n",
    "df2['TypeofDAY_number'] = np.where(df2['dateTime_column'].dt.dayofweek < 5, 1, 0) # 1 for weekday, 0 for weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Date'] = df2['dateTime_column'].dt.strftime('%Y-%m-%d')\n",
    "# df2['Date'] = pd.to_datetime(df2['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dates = {k: v for k, v in df2.groupby('Date')}\n",
    "dict_of_day_type = {k:v for k,v in df2.groupby('TypeofDAY')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_key_value = collections.OrderedDict(dict_of_dates)\n",
    "day_type_key_value = collections.OrderedDict(dict_of_day_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in day_type_key_value:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
