{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_y(x):\n",
    "    y = x**2 - 4*x + 2\n",
    "    return y\n",
    "def gradient_descent(previous_x, learning_rate, epoch):\n",
    "    \n",
    "    # To fill with values\n",
    "    x_gd = []\n",
    "    y_gd = []\n",
    "    \n",
    "    x_gd.append(previous_x)\n",
    "    y_gd.append(func_y(previous_x))\n",
    "    # begin the loops to update x and y with out cost function\n",
    "    for i in range(epoch):\n",
    "        current_x = previous_x - learning_rate * (2*previous_x - 4)\n",
    "        x_gd.append(current_x)\n",
    "        y_gd.append(func_y(current_x))\n",
    "        # update previous_x\n",
    "        previous_x = current_x\n",
    "    return x_gd, y_gd\n",
    "# Initialize x0 and learning rate\n",
    "x0 = 4 # Our first 'guess' at what theta could be\n",
    "learning_rate = 0.15 # Alpha\n",
    "epoch = 100 # Number of tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = gradient_descent(x0, learning_rate, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3.4, 2.98, 2.686, 2.4802, 2.33614, 2.235298, 2.1647086, 2.11529602, 2.0807072140000002, 2.0564950498, 2.03954653486, 2.027682574402, 2.0193778020814, 2.01356446145698, 2.009495123019886, 2.0066465861139204, 2.004652610279744, 2.003256827195821, 2.0022797790370745, 2.0015958453259524, 2.0011170917281667, 2.0007819642097164, 2.0005473749468017, 2.0003831624627613, 2.000268213723933, 2.000187749606753, 2.000131424724727, 2.000091997307309, 2.0000643981151165, 2.0000450786805817, 2.000031555076407, 2.000022088553485, 2.0000154619874393, 2.0000108233912073, 2.000007576373845, 2.0000053034616916, 2.000003712423184, 2.000002598696229, 2.0000018190873603, 2.000001273361152, 2.0000008913528067, 2.0000006239469648, 2.000000436762875, 2.0000003057340128, 2.000000214013809, 2.000000149809666, 2.0000001048667664, 2.0000000734067367, 2.0000000513847156, 2.000000035969301, 2.000000025178511, 2.0000000176249575, 2.00000001233747, 2.000000008636229, 2.00000000604536, 2.0000000042317523, 2.0000000029622265, 2.0000000020735587, 2.000000001451491, 2.0000000010160437, 2.0000000007112306, 2.0000000004978613, 2.000000000348503, 2.000000000243952, 2.0000000001707665, 2.0000000001195364, 2.0000000000836753, 2.0000000000585727, 2.000000000041001, 2.0000000000287006, 2.0000000000200906, 2.0000000000140634, 2.0000000000098446, 2.0000000000068914, 2.000000000004824, 2.000000000003377, 2.000000000002364, 2.0000000000016547, 2.000000000001158, 2.000000000000811, 2.0000000000005675, 2.0000000000003975, 2.000000000000278, 2.0000000000001945, 2.0000000000001363, 2.0000000000000955, 2.0000000000000666, 2.0000000000000466, 2.000000000000033, 2.000000000000023, 2.000000000000016, 2.000000000000011, 2.000000000000008, 2.0000000000000058, 2.000000000000004, 2.0000000000000027, 2.0000000000000018, 2.0000000000000013, 2.000000000000001, 2.0000000000000004]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -0.040000000000000924, -1.0396, -1.5294040000000004, -1.7694079599999997, -1.8870099003999998, -1.9446348511959997, -1.9728710770860403, -1.9867068277721591, -1.9934863456083578, -1.9968083093480953, -1.9984360715805671, -1.9992336750744775, -1.999624500786494, -1.999816005385382, -1.9999098426388375, -1.99995582289303, -1.9999783532175845, -1.9999893930766168, -1.9999948026075423, -1.9999974532776958, -1.9999987521060705, -1.9999993885319745, -1.999999700380668, -1.9999998531865275, -1.9999999280613983, -1.999999964750085, -1.9999999827275419, -1.9999999915364954, -1.9999999958528827, -1.9999999979679126, -1.9999999990042774, -1.999999999512096, -1.9999999997609272, -1.9999999998828546, -1.9999999999425988, -1.9999999999718732, -1.9999999999862181, -1.9999999999932472, -1.9999999999966906, -1.9999999999983782, -1.999999999999205, -1.999999999999611, -1.999999999999809, -1.9999999999999067, -1.9999999999999538, -1.9999999999999778, -1.9999999999999893, -1.9999999999999947, -1.9999999999999973, -1.9999999999999991, -1.9999999999999991, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the idea taken from https://towardsdatascience.com/implementation-of-multi-variate-linear-regression-in-python-using-gradient-descent-optimization-b02f386425b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[1,2,3,4],[8,5,2,5],[6,3,9,7]]\n",
    "A = np.array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [8, 5, 2, 5],\n",
       "       [6, 3, 9, 7]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "X_train = A[:,:-1]\n",
    "y_train = A[:,-1]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_iters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, alpha, num_iters):\n",
    "    n = X.shape[1] # n is number of feature\n",
    "    print('number of feature: ',n)\n",
    "    one_column = np.ones((X.shape[0],1))\n",
    "    print('one column :', one_column)\n",
    "    print('X is: ',X)\n",
    "    X = np.concatenate((one_column, X), axis = 1)\n",
    "    print('now X: ',X)\n",
    "    # initializing the parameter vector...\n",
    "    theta = np.zeros(n+1)\n",
    "    print('theta: ', theta)\n",
    "    # hypothesis calculation....\n",
    "    h = hypothesis(theta, X, n)\n",
    "    # returning the optimized parameters by Gradient Descent...\n",
    "    theta, cost = BGD(theta,alpha,num_iters,h,X,y,n)\n",
    "    return theta, cost\n",
    "\n",
    "def BGD(theta, alpha, num_iters, h, X, y, n):\n",
    "    print('coming input in BGD: theta',theta,'\\n h: ',h,'\\n y: ',y,'\\n X shape: ', X.shape)\n",
    "    print('after getting now h here BGD func will work')\n",
    "    cost = np.ones(num_iters)\n",
    "#     print('cost: ', cost)\n",
    "    for i in range(0,num_iters):\n",
    "        print('---------here h and y : ', h,'\\t',y)\n",
    "        print(' h-y: ', h-y)\n",
    "        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)\n",
    "        print('theta o in BGD: ', theta[0])\n",
    "        for j in range(1,n+1):\n",
    "            print('j: ',j, ' ,X transpose: ', X.transpose()[j])\n",
    "            print('shape',X.transpose()[j].shape)\n",
    "            print('alpha/X.shape[0]',(alpha/X.shape[0]))\n",
    "            print('sum((h-y) * X.transpose()[j]', sum((h-y) * X.transpose()[j]))\n",
    "            theta[j] = theta[j] - (alpha/X.shape[0]) * sum((h-y) * X.transpose()[j])\n",
    "            print('theta j in BGD: ', theta[j])\n",
    "        h = hypothesis(theta, X, n)\n",
    "        cost[i] = (1/X.shape[0]) * 0.5 * sum(np.square(h - y))\n",
    "        print('i: ',i,' ,COST -------', cost[i])\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    return theta, cost\n",
    "\n",
    "def hypothesis(theta, X, n):\n",
    "    print('coming theta: ', theta)\n",
    "    print('\\n now hypothesis function')\n",
    "    h = np.ones((X.shape[0],1))\n",
    "    print('h: ',h)\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    print('reshaped theta', theta)\n",
    "    for i in range(0,X.shape[0]):\n",
    "        h[i] = float(np.matmul(theta, X[i]))\n",
    "    h = h.reshape(X.shape[0])\n",
    "    print('now h: ',h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of feature:  3\n",
      "one column : [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "X is:  [[1 2 3]\n",
      " [8 5 2]\n",
      " [6 3 9]]\n",
      "now X:  [[1. 1. 2. 3.]\n",
      " [1. 8. 5. 2.]\n",
      " [1. 6. 3. 9.]]\n",
      "theta:  [0. 0. 0. 0.]\n",
      "coming theta:  [0. 0. 0. 0.]\n",
      "\n",
      " now hypothesis function\n",
      "h:  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "reshaped theta [[0. 0. 0. 0.]]\n",
      "now h:  [0. 0. 0.]\n",
      "coming input in BGD: theta [0. 0. 0. 0.] \n",
      " h:  [0. 0. 0.] \n",
      " y:  [4 5 7] \n",
      " X shape:  (3, 4)\n",
      "after getting now h here BGD func will work\n",
      "---------here h and y :  [0. 0. 0.] \t [4 5 7]\n",
      " h-y:  [-4. -5. -7.]\n",
      "theta o in BGD:  0.0005333333333333334\n",
      "j:  1  ,X transpose:  [1. 8. 6.]\n",
      "shape (3,)\n",
      "alpha/X.shape[0] 3.3333333333333335e-05\n",
      "sum((h-y) * X.transpose()[j] -86.0\n",
      "theta j in BGD:  0.0028666666666666667\n",
      "j:  2  ,X transpose:  [2. 5. 3.]\n",
      "shape (3,)\n",
      "alpha/X.shape[0] 3.3333333333333335e-05\n",
      "sum((h-y) * X.transpose()[j] -54.0\n",
      "theta j in BGD:  0.0018000000000000002\n",
      "j:  3  ,X transpose:  [3. 2. 9.]\n",
      "shape (3,)\n",
      "alpha/X.shape[0] 3.3333333333333335e-05\n",
      "sum((h-y) * X.transpose()[j] -85.0\n",
      "theta j in BGD:  0.0028333333333333335\n",
      "coming theta:  [0.00053333 0.00286667 0.0018     0.00283333]\n",
      "\n",
      " now hypothesis function\n",
      "h:  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "reshaped theta [[0.00053333 0.00286667 0.0018     0.00283333]]\n",
      "now h:  [0.0155     0.03813333 0.04863333]\n",
      "i:  0  ,COST ------- 14.802976600370371\n",
      "---------here h and y :  [0.0155     0.03813333 0.04863333] \t [4 5 7]\n",
      " h-y:  [-3.9845     -4.96186667 -6.95136667]\n",
      "theta o in BGD:  0.0010632577777777777\n",
      "j:  1  ,X transpose:  [1. 8. 6.]\n",
      "shape (3,)\n",
      "alpha/X.shape[0] 3.3333333333333335e-05\n",
      "sum((h-y) * X.transpose()[j] -85.38763333333333\n",
      "theta j in BGD:  0.005712921111111111\n",
      "j:  2  ,X transpose:  [2. 5. 3.]\n",
      "shape (3,)\n",
      "alpha/X.shape[0] 3.3333333333333335e-05\n",
      "sum((h-y) * X.transpose()[j] -53.63243333333333\n",
      "theta j in BGD:  0.003587747777777778\n",
      "j:  3  ,X transpose:  [3. 2. 9.]\n",
      "shape (3,)\n",
      "alpha/X.shape[0] 3.3333333333333335e-05\n",
      "sum((h-y) * X.transpose()[j] -84.43953333333333\n",
      "theta j in BGD:  0.005647984444444445\n",
      "coming theta:  [0.00106326 0.00571292 0.00358775 0.00564798]\n",
      "\n",
      " now hypothesis function\n",
      "h:  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "reshaped theta [[0.00106326 0.00571292 0.00358775 0.00564798]]\n",
      "now h:  [0.03089563 0.07600133 0.09693589]\n",
      "i:  1  ,COST ------- 14.608641085572803\n",
      "---------here h and y :  [0.03089563 0.07600133 0.09693589] \t [4 5 7]\n",
      " h-y:  [-3.96910437 -4.92399867 -6.90306411]\n",
      "theta o in BGD:  0.0015897966827777775\n",
      "j:  1  ,X transpose:  [1. 8. 6.]\n",
      "shape (3,)\n",
      "alpha/X.shape[0] 3.3333333333333335e-05\n",
      "sum((h-y) * X.transpose()[j] -84.77947836999999\n",
      "theta j in BGD:  0.008538903723444444\n",
      "j:  2  ,X transpose:  [2. 5. 3.]\n",
      "shape (3,)\n",
      "alpha/X.shape[0] 3.3333333333333335e-05\n",
      "sum((h-y) * X.transpose()[j] -53.26739440888889\n",
      "theta j in BGD:  0.005363327591407408\n",
      "j:  3  ,X transpose:  [3. 2. 9.]\n",
      "shape (3,)\n",
      "alpha/X.shape[0] 3.3333333333333335e-05\n",
      "sum((h-y) * X.transpose()[j] -83.88288745777778\n",
      "theta j in BGD:  0.008444080693037037\n",
      "coming theta:  [0.0015898  0.0085389  0.00536333 0.00844408]\n",
      "\n",
      " now hypothesis function\n",
      "h:  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "reshaped theta [[0.0015898  0.0085389  0.00536333 0.00844408]]\n",
      "now h:  [0.0461876  0.11360583 0.14490993]\n",
      "i:  2  ,COST ------- 14.416956738851766\n"
     ]
    }
   ],
   "source": [
    "theta, cost = linear_regression(X_train, y_train,learning_rate, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10757226 0.34290973 0.2842662  0.47555341]]\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array([2,2,3])\n",
    "x=x.reshape(3,)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.8029766  14.60864109 14.41695674 14.22788735 14.04139718 13.85745102\n",
      " 13.67601411 13.49705217 13.3205314  13.14641844 12.97468042 12.80528488\n",
      " 12.63819983 12.47339371 12.31083538 12.15049416 11.99233974 11.83634226\n",
      " 11.68247225 11.53070066 11.38099883 11.23333847 11.0876917  10.94403103\n",
      " 10.80232931 10.6625598  10.5246961  10.38871218 10.25458235 10.1222813\n",
      "  9.99178405  9.86306596  9.73610272  9.61087036  9.48734525  9.36550406\n",
      "  9.2453238   9.12678177  9.0098556   8.89452322  8.78076286  8.66855305\n",
      "  8.55787261  8.44870065  8.34101656  8.23480003  8.13003101  8.02668972\n",
      "  7.92475668  7.82421263  7.72503862  7.62721592  7.53072609  7.43555092\n",
      "  7.34167244  7.24907296  7.157735    7.06764133  6.97877496  6.89111912\n",
      "  6.80465729  6.71937315  6.63525062  6.55227383  6.47042714  6.38969511\n",
      "  6.31006252  6.23151435  6.15403579  6.07761224  6.00222928  5.9278727\n",
      "  5.85452849  5.78218282  5.71082205  5.64043273  5.5710016   5.50251556\n",
      "  5.43496172  5.36832733  5.30259985  5.23776689  5.17381622  5.11073581\n",
      "  5.04851377  4.98713838  4.92659806  4.86688143  4.80797722  4.74987436\n",
      "  4.69256188  4.63602901  4.58026509  4.52525962  4.47100224  4.41748275\n",
      "  4.36469105  4.31261723  4.26125146  4.21058409  4.16060557  4.1113065\n",
      "  4.0626776   4.01470973  3.96739384  3.92072105  3.87468257  3.82926974\n",
      "  3.78447401  3.74028696  3.69670028  3.65370577  3.61129534  3.56946102\n",
      "  3.52819495  3.48748936  3.4473366   3.40772912  3.36865949  3.33012034\n",
      "  3.29210445  3.25460467  3.21761395  3.18112533  3.14513198  3.10962711\n",
      "  3.07460407  3.04005628  3.00597724  2.97236056  2.93919993  2.90648911\n",
      "  2.87422198  2.84239247  2.8109946   2.78002249  2.74947032  2.71933235\n",
      "  2.68960295  2.66027652  2.63134756  2.60281066  2.57466045  2.54689167\n",
      "  2.5194991   2.4924776   2.46582212  2.43952765  2.41358927  2.38800212\n",
      "  2.36276141  2.33786239  2.31330042  2.28907089  2.26516926  2.24159106\n",
      "  2.21833187  2.19538734  2.17275318  2.15042514  2.12839905  2.1066708\n",
      "  2.0852363   2.06409156  2.04323263  2.02265559  2.0023566   1.98233188\n",
      "  1.96257767  1.94309028  1.92386608  1.90490147  1.88619291  1.8677369\n",
      "  1.84953001  1.83156882  1.81384999  1.79637021  1.77912622  1.7621148\n",
      "  1.74533277  1.72877702  1.71244444  1.696332    1.6804367   1.66475556\n",
      "  1.64928567  1.63402414  1.61896814  1.60411486  1.58946154  1.57500543\n",
      "  1.56074387  1.54667418  1.53279376  1.51910003  1.50559043  1.49226245\n",
      "  1.47911362  1.46614149  1.45334366  1.44071775  1.4282614   1.41597231\n",
      "  1.40384821  1.39188682  1.38008595  1.3684434   1.356957    1.34562464\n",
      "  1.33444421  1.32341363  1.31253087  1.30179391  1.29120076  1.28074947\n",
      "  1.27043808  1.26026471  1.25022746  1.24032449  1.23055395  1.22091405\n",
      "  1.211403    1.20201905  1.19276047  1.18362554  1.17461258  1.16571993\n",
      "  1.15694595  1.14828902  1.13974754  1.13131994  1.12300467  1.1148002\n",
      "  1.10670502  1.09871764  1.09083659  1.08306042  1.0753877   1.06781702\n",
      "  1.060347    1.05297625  1.04570344  1.03852721  1.03144626  1.0244593\n",
      "  1.01756503  1.0107622   1.00404956  0.99742588  0.99088995  0.98444058\n",
      "  0.97807658  0.9717968   0.96560008  0.9594853   0.95345134  0.94749709\n",
      "  0.94162148  0.93582344  0.9301019   0.92445582  0.91888419  0.91338597\n",
      "  0.90796018  0.90260583  0.89732195  0.89210757  0.88696175  0.88188357\n",
      "  0.87687208  0.8719264   0.86704562  0.86222887  0.85747526  0.85278394\n",
      "  0.84815407  0.8435848   0.83907531  0.8346248   0.83023245  0.82589747\n",
      "  0.82161909  0.81739654  0.81322905  0.80911588  0.80505628  0.80104954\n",
      "  0.79709493  0.79319174  0.78933927  0.78553684  0.78178376  0.77807937\n",
      "  0.77442299  0.77081398  0.7672517   0.76373551  0.76026477  0.75683889\n",
      "  0.75345724  0.75011922  0.74682425  0.74357174  0.7403611   0.73719178\n",
      "  0.7340632   0.73097483  0.7279261   0.72491649  0.72194546  0.71901249\n",
      "  0.71611705  0.71325865  0.71043678  0.70765094  0.70490065  0.70218542\n",
      "  0.69950477  0.69685824  0.69424536  0.69166568  0.68911875  0.68660411\n",
      "  0.68412135  0.68167001  0.67924968  0.67685994  0.67450037  0.67217056\n",
      "  0.6698701   0.66759861  0.66535569  0.66314094  0.660954    0.65879447\n",
      "  0.656662    0.65455621  0.65247675  0.65042325  0.64839536  0.64639275\n",
      "  0.64441506  0.64246195  0.6405331   0.63862818  0.63674686  0.63488883\n",
      "  0.63305376  0.63124135  0.6294513   0.62768329  0.62593703  0.62421223\n",
      "  0.6225086   0.62082585  0.61916369  0.61752186  0.61590007  0.61429805\n",
      "  0.61271554  0.61115228  0.60960799  0.60808244  0.60657536  0.60508651\n",
      "  0.60361563  0.60216249  0.60072685  0.59930846  0.59790711  0.59652255\n",
      "  0.59515457  0.59380293  0.59246742  0.59114782  0.58984392  0.5885555\n",
      "  0.58728236  0.58602429  0.58478108  0.58355254  0.58233847  0.58113867\n",
      "  0.57995296  0.57878113  0.57762302  0.57647842  0.57534717  0.57422907\n",
      "  0.57312396  0.57203165  0.57095198  0.56988478  0.56882988  0.56778711\n",
      "  0.56675631  0.56573732  0.56472998  0.56373414  0.56274964  0.56177632\n",
      "  0.56081405  0.55986266  0.55892202  0.55799198  0.5570724   0.55616314\n",
      "  0.55526405  0.55437501  0.55349588  0.55262653  0.55176682  0.55091663\n",
      "  0.55007583  0.54924429  0.54842189  0.5476085   0.54680402  0.54600831\n",
      "  0.54522127  0.54444277  0.5436727   0.54291095  0.54215741  0.54141196\n",
      "  0.54067451  0.53994494  0.53922315  0.53850903  0.53780249  0.53710342\n",
      "  0.53641172  0.53572729  0.53505005  0.53437988  0.53371671  0.53306043\n",
      "  0.53241095  0.53176818  0.53113204  0.53050244  0.52987929  0.5292625\n",
      "  0.52865199  0.52804767  0.52744947  0.52685731  0.52627109  0.52569075\n",
      "  0.52511621  0.52454739  0.52398421  0.52342661  0.5228745   0.52232781\n",
      "  0.52178647  0.52125042  0.52071958  0.52019388  0.51967326  0.51915765\n",
      "  0.51864698  0.51814119  0.51764021  0.51714398  0.51665244  0.51616552\n",
      "  0.51568317  0.51520533  0.51473193  0.51426292  0.51379825  0.51333784\n",
      "  0.51288166  0.51242963  0.51198172  0.51153786  0.511098    0.51066209\n",
      "  0.51023008  0.50980191  0.50937754  0.50895692  0.50854     0.50812673\n",
      "  0.50771706  0.50731095  0.50690835  0.50650921  0.50611349  0.50572115\n",
      "  0.50533213  0.50494641  0.50456393  0.50418465  0.50380854  0.50343555\n",
      "  0.50306564  0.50269876  0.5023349   0.50197399  0.50161601  0.50126092\n",
      "  0.50090868  0.50055925  0.50021259  0.49986868  0.49952748  0.49918895\n",
      "  0.49885306  0.49851977  0.49818905  0.49786087  0.49753519  0.49721199\n",
      "  0.49689123  0.49657288  0.49625691  0.49594329  0.49563199  0.49532298\n",
      "  0.49501624  0.49471172  0.49440941  0.49410928  0.49381129  0.49351543\n",
      "  0.49322166  0.49292997  0.49264031  0.49235267  0.49206702  0.49178334\n",
      "  0.49150161  0.49122179  0.49094386  0.4906678   0.49039359  0.4901212\n",
      "  0.48985061  0.48958181  0.48931475  0.48904943  0.48878583  0.48852392\n",
      "  0.48826367  0.48800508  0.48774811  0.48749276  0.48723899  0.4869868\n",
      "  0.48673615  0.48648704  0.48623944  0.48599333  0.4857487   0.48550552\n",
      "  0.48526379  0.48502348  0.48478457  0.48454705  0.4843109   0.48407611\n",
      "  0.48384265  0.48361052  0.48337969  0.48315015  0.48292188  0.48269488\n",
      "  0.48246912  0.48224458  0.48202126  0.48179914  0.48157821  0.48135845\n",
      "  0.48113984  0.48092238  0.48070605  0.48049083  0.48027672  0.48006369\n",
      "  0.47985174  0.47964086  0.47943103  0.47922224  0.47901448  0.47880773\n",
      "  0.47860198  0.47839723  0.47819346  0.47799065  0.4777888   0.4775879\n",
      "  0.47738794  0.4771889   0.47699077  0.47679354  0.47659721  0.47640176\n",
      "  0.47620719  0.47601347  0.47582061  0.47562859  0.47543741  0.47524705\n",
      "  0.4750575   0.47486876  0.47468082  0.47449366  0.47430728  0.47412167\n",
      "  0.47393683  0.47375273  0.47356938  0.47338676  0.47320488  0.47302371\n",
      "  0.47284325  0.4726635   0.47248445  0.47230608  0.4721284   0.47195138\n",
      "  0.47177504  0.47159935  0.47142432  0.47124993  0.47107617  0.47090305\n",
      "  0.47073056  0.47055868  0.47038741  0.47021674  0.47004668  0.4698772\n",
      "  0.46970831  0.46954     0.46937227  0.4692051   0.46903849  0.46887244\n",
      "  0.46870693  0.46854197  0.46837755  0.46821366  0.4680503   0.46788746\n",
      "  0.46772514  0.46756332  0.46740202  0.46724121  0.4670809   0.46692108\n",
      "  0.46676175  0.46660289  0.46644452  0.46628661  0.46612917  0.4659722\n",
      "  0.46581568  0.46565961  0.46550399  0.46534882  0.46519409  0.46503979\n",
      "  0.46488592  0.46473248  0.46457946  0.46442686  0.46427468  0.46412291\n",
      "  0.46397154  0.46382058  0.46367002  0.46351985  0.46337008  0.46322069\n",
      "  0.46307169  0.46292307  0.46277483  0.46262696  0.46247946  0.46233233\n",
      "  0.46218556  0.46203916  0.46189311  0.46174742  0.46160207  0.46145708\n",
      "  0.46131243  0.46116812  0.46102415  0.46088051  0.46073721  0.46059424\n",
      "  0.4604516   0.46030927  0.46016728  0.4600256   0.45988423  0.45974318\n",
      "  0.45960244  0.45946201  0.45932188  0.45918206  0.45904253  0.45890331\n",
      "  0.45876437  0.45862574  0.45848739  0.45834933  0.45821156  0.45807407\n",
      "  0.45793686  0.45779993  0.45766327  0.4575269   0.45739079  0.45725495\n",
      "  0.45711938  0.45698408  0.45684904  0.45671427  0.45657975  0.45644549\n",
      "  0.45631149  0.45617774  0.45604425  0.455911    0.455778    0.45564525\n",
      "  0.45551274  0.45538048  0.45524845  0.45511667  0.45498512  0.45485381\n",
      "  0.45472274  0.45459189  0.45446128  0.45433089  0.45420074  0.45407081\n",
      "  0.4539411   0.45381162  0.45368235  0.45355331  0.45342449  0.45329588\n",
      "  0.45316749  0.45303931  0.45291134  0.45278359  0.45265604  0.4525287\n",
      "  0.45240157  0.45227465  0.45214792  0.45202141  0.45189509  0.45176897\n",
      "  0.45164305  0.45151733  0.45139181  0.45126648  0.45114135  0.45101641\n",
      "  0.45089166  0.4507671   0.45064273  0.45051854  0.45039455  0.45027074\n",
      "  0.45014711  0.45002367  0.44990041  0.44977734  0.44965444  0.44953172\n",
      "  0.44940918  0.44928682  0.44916464  0.44904263  0.44892079  0.44879913\n",
      "  0.44867764  0.44855632  0.44843517  0.4483142   0.44819339  0.44807274\n",
      "  0.44795227  0.44783196  0.44771182  0.44759184  0.44747202  0.44735237\n",
      "  0.44723287  0.44711354  0.44699437  0.44687536  0.4467565   0.44663781\n",
      "  0.44651927  0.44640088  0.44628265  0.44616458  0.44604666  0.44592889\n",
      "  0.44581127  0.44569381  0.44557649  0.44545933  0.44534231  0.44522545\n",
      "  0.44510873  0.44499216  0.44487573  0.44475945  0.44464332  0.44452733\n",
      "  0.44441148  0.44429578  0.44418022  0.4440648   0.44394952  0.44383439\n",
      "  0.44371939  0.44360453  0.44348981  0.44337523  0.44326079  0.44314649\n",
      "  0.44303232  0.44291828  0.44280438  0.44269062  0.44257699  0.44246349\n",
      "  0.44235013  0.4422369   0.4421238   0.44201084  0.441898    0.44178529\n",
      "  0.44167272  0.44156027  0.44144795  0.44133576  0.4412237   0.44111177\n",
      "  0.44099996  0.44088828  0.44077672  0.44066529  0.44055399  0.44044281\n",
      "  0.44033175  0.44022082  0.44011001  0.43999932  0.43988875  0.43977831\n",
      "  0.43966799  0.43955779  0.43944771  0.43933775  0.43922791  0.43911818\n",
      "  0.43900858  0.4388991   0.43878973  0.43868048  0.43857135  0.43846233\n",
      "  0.43835343  0.43824465  0.43813598  0.43802743  0.43791899  0.43781067\n",
      "  0.43770246  0.43759436  0.43748638  0.43737851  0.43727075  0.43716311\n",
      "  0.43705558  0.43694815  0.43684084  0.43673364  0.43662656  0.43651958\n",
      "  0.43641271  0.43630595  0.4361993   0.43609275  0.43598632  0.43588\n",
      "  0.43577378  0.43566767  0.43556166  0.43545577  0.43534998  0.4352443\n",
      "  0.43513872  0.43503325  0.43492788  0.43482262  0.43471746  0.43461241\n",
      "  0.43450746  0.43440261  0.43429787  0.43419323  0.4340887   0.43398426\n",
      "  0.43387993  0.4337757   0.43367158  0.43356755  0.43346363  0.4333598\n",
      "  0.43325608  0.43315246  0.43304894  0.43294551  0.43284219  0.43273897\n",
      "  0.43263584  0.43253282  0.43242989  0.43232706  0.43222433  0.4321217\n",
      "  0.43201916  0.43191672  0.43181438  0.43171213  0.43160999  0.43150793\n",
      "  0.43140598  0.43130412  0.43120235  0.43110068  0.43099911  0.43089763\n",
      "  0.43079624  0.43069495  0.43059376  0.43049265  0.43039165  0.43029073\n",
      "  0.43018991  0.43008918  0.42998854  0.429888    0.42978755  0.42968719\n",
      "  0.42958692  0.42948674  0.42938666  0.42928666  0.42918676  0.42908695\n",
      "  0.42898723  0.4288876   0.42878806  0.42868861  0.42858925  0.42848997\n",
      "  0.42839079  0.4282917   0.4281927   0.42809378  0.42799495  0.42789622\n",
      "  0.42779757  0.427699    0.42760053  0.42750214  0.42740384  0.42730563\n",
      "  0.42720751  0.42710947  0.42701152  0.42691365  0.42681587  0.42671818\n",
      "  0.42662057  0.42652305  0.42642561  0.42632826  0.42623099  0.42613381\n",
      "  0.42603672  0.4259397   0.42584278  0.42574593]\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cost')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt4XXWd7/H3d++de5MmadJL0tK0pbSFAgUDQkVBwDmACHhDOF5A0J4zj3NER4eR43NGnXlGz8io6HjtUQQdHhhEvBw8ggg4lYtAgLa0lNICpfc26TW95Lq/54+1ku6GNE3T7L2y9/q8nmc/e6+1V9bvu7Ign/7W5bfM3RERkfhKRF2AiIhES0EgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYi4VdQHDUVdX501NTVGXISKSV5577rk2d68/2nJ5EQRNTU20tLREXYaISF4xszeGs5wODYmIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwUdBI++vI3v/2lt1GWIiIxpBR0Ej6/ZwXcfXYueyywicmQFHQQN1aUc6Oplz8HuqEsRERmzCjoIptaUAbBx18GIKxERGbsKOggaqoMg2LxbQSAiciQFHQSNYRBsUhCIiBxRQQdBbUUxJamEegQiIkMo6CAwMxqry9QjEBEZQkEHAUBjTRmbdndEXYaIyJiVtSAws9vNbLuZrRjku8+bmZtZXbba79MwvkyHhkREhpDNHsEdwCUDZ5rZNOBdwPostt2vobqM1vZOOrp7c9GciEjeyVoQuPsSYOcgX30LuBnIye2+jeG9BFv36PCQiMhgcnqOwMyuADa5+7JhLLvIzFrMrKW1tXXEbTZUlwK6hFRE5EhyFgRmVg58EfiH4Szv7ovdvdndm+vr60fcru4lEBEZWi57BLOAGcAyM1sHTAWeN7PJ2Wx08vhSzGCThpkQERlUKlcNufuLwMS+6TAMmt29LZvtlqSS1I8r0ZVDIiJHkM3LR+8GngLmmNlGM7sxW20dTWNNGZv3KAhERAaTtR6Bu197lO+bstX2QA3VZazctCdXzYmI5JWCv7MYghPGm/d0kE7rATUiIgPFJgi6etK07euMuhQRkTEnFkFwQm05ABt2HYi4EhGRsScWQTCtNriXYP1OBYGIyECxCIKpNWGPYKeuHBIRGSgWQVBalGRiZYl6BCIig4hFEEBwnmCDgkBE5E1iEwTTFAQiIoOKVRBs2dtBV0866lJERMaU+ARBTRnuaMwhEZEBYhMEffcS6ISxiMjhYhME03RTmYjIoGITBJOqSilOJtQjEBEZIDZBkEwYjTVlbNRNZSIih4lNEEBweEg9AhGRw8UqCE6oLdM5AhGRAWIVBNNqytl9oJu9Hd1RlyIiMmbEKgj6LyHdoV6BiEifbD6z+HYz225mKzLm3WpmL5vZcjP7lZlVZ6v9wUyfUAHAGwoCEZF+2ewR3AFcMmDew8B8dz8NeAW4JYvtv0lTXdAjWLdjfy6bFREZ07IWBO6+BNg5YN4f3L0nnPwLMDVb7Q+mvDjFpKoSXm9TEIiI9InyHMENwO9z3ej0CRWsUxCIiPSLJAjM7ItAD3DXEMssMrMWM2tpbW0dtbZnTKjQoSERkQw5DwIzuw64HPiwu/uRlnP3xe7e7O7N9fX1o9Z+U10Fbfu6aNclpCIiQI6DwMwuAf4euMLdI7l0Z0bfCeM2XTkkIgLZvXz0buApYI6ZbTSzG4HvApXAw2a21Mx+mK32j6SpLriEVIeHREQCqWyt2N2vHWT2T7LV3nBNrw2DQCeMRUSAmN1ZDFBWnGRyVSmvq0cgIgLEMAgguLFMPQIRkUAsg2BGXQXrNMyEiAgQ0yBomlDBzv1d7DmoS0hFROIZBHU6YSwi0ieWQTAzDAKNOSQiEtMgOGFCOcmE8WrrvqhLERGJXCyDoCSV5ITactZuVxCIiMQyCABm1Y9Tj0BEhDgHwcQKXm/bT09vOupSREQiFdsgOLF+HN29zoZdB6MuRUQkUvENgonjAHSeQERiL7ZBMCsMAp0nEJG4i20QVJUWMbGyRD0CEYm92AYB6MohERGIeRCcOHEca7fvY4gnZoqIFLzYB0F7Rw+t+zqjLkVEJDKxDoJZ9bpySEQk1kHQdwnpqwoCEYmxbD68/nYz225mKzLm1ZrZw2a2JnyvyVb7wzGpqoTK0hSrt7VHWYaISKSy2SO4A7hkwLwvAI+4+2zgkXA6MmbGnEmVvLJVPQIRia+sBYG7LwF2Dph9JXBn+PlO4KpstT9ccyZX8vLWvbpySERiK9fnCCa5+xaA8H3ikRY0s0Vm1mJmLa2trVkraO7kSvZ29LB1b0fW2hARGcvG7Mlid1/s7s3u3lxfX5+1dk6aVAnA6q06TyAi8ZTrINhmZlMAwvftOW7/TeZOrgIUBCISX7kOgt8C14WfrwN+k+P232R8eRGTq0oVBCISW9m8fPRu4ClgjpltNLMbgf8NvMvM1gDvCqcjd9LkSl5WEIhITKWytWJ3v/YIX12UrTZHau7kSu54bQc9vWlSyTF72kREJCv0V4/ghHFXT5p1Ow5EXYqISM4pCAh6BKATxiISTwoCgjGHEgart+6NuhQRkZxTEAClRUlm1FXw0hb1CEQkfhQEoVMaxvPS5j1RlyEiknMKgtApDVVs3tPBrv1dUZciIpJTCoLQKQ3jAVi5WecJRCReFAShUxqCoSZW6PCQiMSMgiBUU1FMY3WZegQiEjsKggwnN1SxUj0CEYkZBUGGUxqqeL1tP/s7e6IuRUQkZxQEGU5pGI87vKwby0QkRhQEGeY3BieMdZ5AROJEQZBhclUptRXFrNik8wQiEh8KggxmxvzG8by4ST0CEYkPBcEAC6aOZ/XWvRzo0gljEYkHBcEAp0+rJu2wQr0CEYkJBcEAp0+rBmDphl0RVyIikhvDCgIz+/lw5g2XmX3WzFaa2Qozu9vMSke6rtFWN66EqTVlLNugE8YiEg/D7RGckjlhZkngLSNp0MwagU8Dze4+H0gC14xkXdmyYFo1SzfsjroMEZGcGDIIzOwWM2sHTjOzveGrHdgO/OY42k0BZWaWAsqBzcexrlG3YFo1m3YfZHt7R9SliIhk3ZBB4O5fc/dK4FZ3rwpfle4+wd1vGUmD7r4J+FdgPbAF2OPufxjJurKl7zyBDg+JSBwM99DQA2ZWAWBmHzGzb5rZ9JE0aGY1wJXADKABqDCzjwyy3CIzazGzltbW1pE0NWLzG8aTTBjLdHhIRGJguEHwA+CAmZ0O3Ay8AfxshG1eDLzu7q3u3g3cDywcuJC7L3b3Zndvrq+vH2FTI1NWnGTOpEqdJxCRWBhuEPS4uxP8S/7b7v5toHKEba4HzjGzcjMz4CJg1QjXlTULTqhm2cbdpNMedSkiIlk13CBoN7NbgI8CvwuvGioaSYPu/jRwH/A88GJYw+KRrCubFkytpr2jh9fa9kVdiohIVg03CD4EdAI3uPtWoBG4daSNuvuX3H2uu89394+6e+dI15Utb2mqAeDZdbqxTEQK27CCIPzjfxcw3swuBzrcfaTnCPLCzLoK6sYV8+zrO6MuRUQkq4Z7Z/HVwDPAB4GrgafN7APZLCxqZkbz9FqeWacgEJHClhrmcl8EznL37QBmVg/8keBYf8E6a0YtD67cypY9B5kyvizqckREsmK45wgSfSEQ2nEMP5u3zm6qBXSeQEQK23D/mD9oZg+Z2fVmdj3wO+D/Za+ssWHelEoqipM6TyAiBW3IQ0NmdiIwyd3/zszeB5wHGPAUwcnjgpZKJjhzeg3P6jyBiBSwo/UIbgPaAdz9fnf/W3f/LEFv4LZsFzcWnN1Uy+pt7ew50B11KSIiWXG0IGhy9+UDZ7p7C9CUlYrGmLNm1OIOLW+oVyAiheloQTDUA2NicRnNgmnVFCVNl5GKSME6WhA8a2afHDjTzG4EnstOSWNLaVGSM6bV8NSrO6IuRUQkK452H8FngF+Z2Yc59Ie/GSgG3pvNwsaSt51Yx22PvMLuA11UlxdHXY6IyKg62oNptrn7QuArwLrw9RV3PzccdiIWzps9AXfUKxCRgjSsO4vd/THgsSzXMmadNrWacSUpHl/bxqWnTom6HBGRUVXwdwePhqJkgnNm1vLE2raoSxERGXUKgmFaOKuOdTsOsGHngahLEREZVQqCYTpvdh0AT76qXoGIFBYFwTDNnjiO+soSHl+rE8YiUlgUBMNkZpx3Yh1Prm3Tc4xFpKAoCI7B22fXsWN/Fys274m6FBGRURNJEJhZtZndZ2Yvm9kqMzs3ijqO1QVzJmIGj6zafvSFRUTyRFQ9gm8DD7r7XOB0YFVEdRyT2opizphWzWOrFQQiUjhyHgRmVgW8A/gJgLt3ufvuXNcxUhfNm8TyjXvYvrcj6lJEREZFFD2CmUAr8FMze8HMfmxmFQMXMrNFZtZiZi2tra25r/II3jlnIoB6BSJSMKIIghRwJvADdz8D2A98YeBC7r7Y3Zvdvbm+vj7XNR7RvCmVTBlfyqMvKwhEpDBEEQQbgY3u/nQ4fR9BMOQFM+PCuRP585o2Ont6oy5HROS45TwIwlFLN5jZnHDWRcBLua7jeFw4dyIHunp5+jU9rEZE8l9UVw39D+AuM1sOLAC+GlEdI7JwVh1lRUn+8FJsRuIWkQIWSRC4+9Lw+P9p7n6Vu++Koo6RKitO8s659Ty4Yhu9ustYRPKc7iweoUvnT6FtXyfP6lnGIpLnFAQjdOHciZSkEvz+xS1RlyIiclwUBCNUUZLigjn1/H7FVg1CJyJ5TUFwHC47dQrb2zt5bn1eneIQETmMguA4XDRvEsWpBL9brsNDIpK/FATHYVxJivNPquf3K7bo6iERyVsKguN01YJGtu3t1CMsRSRvKQiO00XzJlJZmuL+5zdFXYqIyIgoCI5TaVGSy09r4MEVW9nX2RN1OSIix0xBMAo+8JZGDnb36p4CEclLCoJRcOYJNTRNKNfhIRHJSwqCUWBmvO/MqTz12g427joQdTkiIsdEQTBK3ntGI2Zwb8vGqEsRETkmCoJRMq22nPNPqueeZ9bT3ZuOuhwRkWFTEIyij7x1OtvbO/njS9uiLkVEZNgUBKPonXMn0lhdxr8//UbUpYiIDJuCYBQlE8a1Z0/jibU7eLV1X9TliIgMi4JglF191jRSCeOuv6yPuhQRkWGJLAjMLGlmL5jZA1HVkA0TK0u59NQp/KJlA3s7uqMuR0TkqKLsEdwErIqw/axZ9PaZtHf2cPfT6hWIyNgXSRCY2VTg3cCPo2g/206dOp6Fsybw0yfW0dWjS0lFZGyLqkdwG3AzULB/JRe9YyZb93bw22Wboy5FRGRIOQ8CM7sc2O7uzx1luUVm1mJmLa2trTmqbvScf1I9cydXsnjJq7jroTUiMnZF0SN4G3CFma0D7gEuNLN/H7iQuy9292Z3b66vr891jcfNzPhv58/klW37eFg3mInIGJbzIHD3W9x9qrs3AdcAj7r7R3JdRy6857QGZtRV8K0/riGtR1mKyBil+wiyKJVMcNNFs1m1ZS8PrtwadTkiIoOKNAjc/U/ufnmUNWTbe05vYPbEcXzr4Vf0gHsRGZPUI8iyZML4zMUnsWb7Ph5YriuIRGTsURDkwKXzJzNvShW3PrSaju7eqMsRETmMgiAHEgnjf717Hht3HeT2J16PuhwRkcMoCHJk4Yl1vOvkSXzv0bVsb++IuhwRkX4Kghz6n5fNo6s3zTceeiXqUkRE+ikIcmhGXQXXL2zi3uc28Pz6XVGXIyICKAhy7qaLT2JyVSm3/PJFDUgnImOCgiDHxpWk+Kcr57N6WzuLl7wadTkiIgqCKFx88iTefeoUvvPoWj3SUkQipyCIyJeuOJnSVILP3buM7l4dIhKR6CgIIjKxspSvvu9Ulm7Yzb89ujbqckQkxhQEEbr8tAbef+ZUvvvoGlrW7Yy6HBGJKQVBxL58xck01pRx0z1L2bW/K+pyRCSGFAQRqywt4t+uPZPW9k4+fc8LGqFURHJOQTAGLJhWzVeuPIU/r2njmw+vjrocEYmZVNQFSODas09g+cbdfO+xV5k3pYrLT2uIuiQRiQn1CMaQL19xCs3Ta/jb/1jGX17bEXU5IhITCoIxpCSV5MfXNTOttoxP/qyF1Vvboy5JRGJAQTDGVJcXc+cNZ1NWlOS6259hw84DUZckIgUu50FgZtPM7DEzW2VmK83splzXMNZNrSnnzhvO5mB3L9cs/gvrdygMRCR7ougR9ACfc/d5wDnAp8zs5AjqGNPmTanirk+8lf1dPVyz+CmFgYhkTc6DwN23uPvz4ed2YBXQmOs68sH8xvHc9Ym3cqC7l6t/9BSrtuyNuiQRKUCRniMwsybgDODpQb5bZGYtZtbS2tqa69LGjFMaxnPPonMAuPqHT/HE2raIKxKRQhNZEJjZOOCXwGfc/U3/1HX3xe7e7O7N9fX1uS9wDJk7uYpffWohDdVlXHf7M9z77IaoSxKRAhJJEJhZEUEI3OXu90dRQ76ZMr6MX/z1uZwzcwI3/3I5t9z/Ip09vVGXJSIFIIqrhgz4CbDK3b+Z6/bzWVVpEXd8/Cz++oJZ3P3Meq7+4VO6vFREjlsUPYK3AR8FLjSzpeHrsgjqyEupZIK/v2QuP/roW3itdT+X3LaEu59Zj7sGqxORkbF8+APS3NzsLS0tUZcx5mzafZCb71vGE2t3cMGcer763lNpqC6LuiwRGSPM7Dl3bz7acrqzOI81Vpfx8xveyleuOIW/vLaDi77xn3z/T2vp6tGjL0Vk+BQEeS6RMK5b2MTDnz2ft8+u4+sPruaS25bw8EvbdLhIRIZFQVAgptWWs/hjzdzx8bMA+OTPWrjq+0/y+Jo2BYKIDEnnCApQT2+aXz6/kW//cQ2b93RwVlMNn3j7TC6eN4lkwqIuT0RyZLjnCBQEBayzp5d7ntnA4iWvsWn3QaZPKOfjC5t4/1umUllaFHV5IpJlCgLp19Ob5qGV2/jx46/xwvrdlBYluHT+FD7YPJVzZkwgoV6CSEFSEMiglm7Yzb0tG/i/yzbT3tFDY3UZl86fzCXzJ3PmCTUKBZECoiCQIXV09/LQyq38+oVNPLF2B129aeorS7h43iTOP6mOc2fWMb5ch49E8pmCQIatvaObx1a38tCKrfxp9Xb2d/WSMDh1ajXnnTiB5qZazphWTXV5cdSlisgxUBDIiHT3plm6YTePr2nj8bVtLN2wm9508N/IzLoKFpxQzRnTqpk3pYqTJldSpZPOImOWgkBGxf7OHpZv3MMLG3bxwvrdvLB+F237uvq/bxhfypzJlcyZXMWJE8cxfUI502vLqa8sIRhfUESiMtwgSOWiGMlfFSUpzp01gXNnTQDA3dm8p4OXt+zl5a3trA5ff17TRk/60D8qyoqSnFBbzvQJ5UytKWdSVQmTx5cyqaqUyVXBe1lxMqrNEpEMCgI5JmZGY3UZjdVlXDRvUv/8rp40G3cd4I2dB1i/4wBv7DjAGzv281rbfv68po2D3W9+dkJVaYr6yhJqK4qpLi+mtryY6ooiasuLqSkvpqaimOryIsaVpBhXkqKyNEVFSYqipG6IFxlNCgIZFcWpBDPrxzGzftybvnN32jt72L63g617Otm6t4Nt4attXye79nezYecBlm/cza793XT1Dj1oXkkqEYRDaRAQFWFQlBYlKE0lKSlKBp+LkpSmDn0uSYXzihKUhNPFyQSpZIKipFGUTJBKBO9F4bxUsm8ZI5UwHe6SgqQgkKwzM6pKi6gqLeLEiZVDLuvuHOjqZef+LnYf6GbXgS72d/bQ3tnD/s4e9nX0sK8z49URfLdtbwcd3b109qTp6E7T2d1LR08v3b2jew4sMzCKUwlSiQTJhJFIQNKMZCJ4JQb7nDkvYSSNNy3bt1wi871/3QkSBmaQsCCUEv2fOWw6EU5bxnQiDLH+6TDYjMOXGbguG/Dev+6+n0v0LX/4uuwI7/1t0NfWoc+HvqO/DuPw9vqyOJGwQ8sRrJuB68pcT/96w3rIWFfG95m/g7hQEMiYYmZUhP/Kn1Z7/OvrTTsd3b3Bqyfd/7mzJ01nd5qedJru3jTdvU53b5qeXqcrfA/mB9/19KbpTvctc2j57t40vWlIu9Obzni5kw7fe9Pe/31POk1nj9PrBN9nfNebsY5DP3v4ut2dtIMTvofTaXfy4LqPvJQ4LJAO/5wYEFQDQ+dQcPWF4eFBRf/PDFhXRmh97X2nclbTKPzPMAQFgRS0ZOJQsMRBZjD0hYNnTKcdGDDt7jj0T6fT4c+RGTJ9yx6+7sz3zGAa+J7ObKN//Yfadj80L+0Dvg9rydwW59B2BMsdqpeMeUOtK53xGcLtZmAbPkhNh9fd93s5VOOb1+Nhwel0xnLhejnKdpfn4KKKePzfIRITZuEhJ+JzWEOOXySXX5jZJWa22szWmtkXoqhBREQCOQ8CM0sC3wMuBU4GrjWzk3Ndh4iIBKLoEZwNrHX319y9C7gHuDKCOkREhGiCoBHYkDG9MZwnIiIRiCIIBjuL9aYL38xskZm1mFlLa2trDsoSEYmnKIJgIzAtY3oqsHngQu6+2N2b3b25vr4+Z8WJiMRNFEHwLDDbzGaYWTFwDfDbCOoQEREiuI/A3XvM7G+Ah4AkcLu7r8x1HSIiEsiL5xGYWSvwxgh/vA5oG8Vy8oG2OR60zfFwPNs83d2Pemw9L4LgeJhZy3AezFBItM3xoG2Oh1xsswZ2FxGJOQWBiEjMxSEIFkddQAS0zfGgbY6HrG9zwZ8jEBGRocWhRyAiIkMo2CAo1KGuzWyamT1mZqvMbKWZ3RTOrzWzh81sTfheE843M/tO+HtYbmZnRrsFI2dmSTN7wcweCKdnmNnT4Tb/R3iDImZWEk6vDb9virLukTKzajO7z8xeDvf3uYW+n83ss+F/1yvM7G4zKy20/Wxmt5vZdjNbkTHvmPermV0XLr/GzK47npoKMggKfKjrHuBz7j4POAf4VLhtXwAecffZwCPhNAS/g9nhaxHwg9yXPGpuAlZlTP8L8K1wm3cBN4bzbwR2ufuJwLfC5fLRt4EH3X0ucDrBthfsfjazRuDTQLO7zye44fQaCm8/3wFcMmDeMe1XM6sFvgS8lWBE5y/1hceIePgYukJ6AecCD2VM3wLcEnVdWdrW3wDvAlYDU8J5U4DV4ecfAddmLN+/XD69CMakegS4EHiAYPDCNiA1cJ8T3LV+bvg5FS5nUW/DMW5vFfD6wLoLeT9zaGTi2nC/PQD8l0Lcz0ATsGKk+xW4FvhRxvzDljvWV0H2CIjJUNdhV/gM4GlgkrtvAQjfJ4aLFcrv4jbgZiAdTk8Adrt7TziduV392xx+vydcPp/MBFqBn4aHw35sZhUU8H52903AvwLrgS0E++05Cns/9znW/Tqq+7tQg2BYQ13nMzMbB/wS+Iy77x1q0UHm5dXvwswuB7a7+3OZswdZ1IfxXb5IAWcCP3D3M4D9HDpcMJi83+bw0MaVwAygAaggODQyUCHt56M50jaO6rYXahAMa6jrfGVmRQQhcJe73x/O3mZmU8LvpwDbw/mF8Lt4G3CFma0jeKLdhQQ9hGoz6xs4MXO7+rc5/H48sDOXBY+CjcBGd386nL6PIBgKeT9fDLzu7q3u3g3cDyyksPdzn2Pdr6O6vws1CAp2qGszM+AnwCp3/2bGV78F+q4cuI7g3EHf/I+FVx+cA+zp64LmC3e/xd2nunsTwb581N0/DDwGfCBcbOA29/0uPhAun1f/UnT3rcAGM5sTzroIeIkC3s8Eh4TOMbPy8L/zvm0u2P2c4Vj360PAX5lZTdiT+qtw3shEfdIkiydjLgNeAV4Fvhh1PaO4XecRdAGXA0vD12UEx0YfAdaE77Xh8kZwBdWrwIsEV2REvh3Hsf0XAA+En2cCzwBrgV8AJeH80nB6bfj9zKjrHuG2LgBawn39a6Cm0Pcz8BXgZWAF8HOgpND2M3A3wTmQboJ/2d84kv0K3BBu+1rg48dTk+4sFhGJuUI9NCQiIsOkIBARiTkFgYhIzCkIRERiTkEgIhJzCgIZ88zMzewbGdOfN7MvZ6GdW8ORL28dMP8KC0ewNbOrRnMAQzNbYGaXDdaWSK7o8lEZ88ysg+C667Pcvc3MPg+Mc/cvj3I7e4F6d+8cYpk7CO5juO8Y1pvyQ2PlDPzueoJrw//mGMsVGTXqEUg+6CF4XN9nB35hZtPN7JFwrPZHzOyEoVYU3qF5azje/Ytm9qFw/m8JxrZ5um9exs9cb2bfNbOFwBXArWa21Mxmha8Hzew5M/uzmc0Nf+YOM/ummT0G/IuZnW1mT4YDyD1pZnPCu97/EfhQuL4P9bU11LaF6/5OuJ7XzOwD4fwpZrYkXNcKM3v7cf3WJTZSR19EZEz4HrDczL4+YP53gZ+5+51mdgPwHeCqIdbzPoI7dk8H6oBnzWyJu19hZvvcfcGRftDdnwwDo79HYGaPAP/d3deY2VuB7xOMhQRwEnCxu/eaWRXwDnfvMbOLga+6+/vN7B/I6BGEPYThbNsUgrvM5xIMQ3Af8F8Jhmj+ZwueyVE+xO9BpJ+CQPKCu+81s58RPLjkYMZX5xL8cYdgSIKBQTHQecDd7t5LMNDXfwJnMYKxqCwYAXYh8ItgaBwgGBKhzy/CdiAYEO1OM5tNMERI0TCaGGrbfu3uaeAlM5sUznsWuN2CQQl/7e5Lj3WbJJ50aEjyyW0E47JUDLHM0U56DTZ870glCMbKX5Dxmpfx/f6Mz/8EPObBk7feQzBOzrHK3LbM8xgG4O5LgHcAm4Cfm9nHRtCGxJCCQPKGu+8E7uXQowoBniQYkRTgw8DjR1nNEoJj8kkzqyf4w/nMMZTRDlSG9ewFXjezD0L/+YfTj/Bz4wn+QANcP9j6BnFM22Zm0wme2/B/CEaozcvnFkvuKQgk33yD4Nh+n08DHzez5cBHCZ5r3HcZ5j8O8vO/IhjNcxnwKHCzB0M+D9c9wN+FJ31nEfyBvtHMlgErCR6sMpivA18zsycInsXb5zHg5L6TxQN+ZtBtG8IFwFIzewF4P8Ezj0WOSpfiYhHkAAAANklEQVSPiojEnHoEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOb+PxI24W4JipPOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cost = list(cost)\n",
    "n_iterations = [x for x in range(1,num_iters+1)]\n",
    "plt.plot(n_iterations, cost)\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(m, b, X, Y, learning_rate):\n",
    "    m_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        # -2x(y - (mx + b))\n",
    "        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))\n",
    "\n",
    "        # -2(y - (mx + b))\n",
    "        b_deriv += -2*(Y[i] - (m*X[i] + b))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    m -= (m_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.6 5.4 8.5]\n",
      "[1.6 1.6 1.6]\n"
     ]
    }
   ],
   "source": [
    "m,b = update_weights(0,0,X_train, y_train, learning_rate)\n",
    "print(m)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDRegressor(max_iter=num_iters, alpha= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.15, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=10, n_iter=None, penalty='l2',\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: [0.27884363 0.32251253 0.48547914]\n",
      "Intercept: [0.14142455]\n"
     ]
    }
   ],
   "source": [
    "print('Slope:' ,clf.coef_)\n",
    "print('Intercept:', clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.linear_model' has no attribute 'GDRegressor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-74262e1ddacf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGDRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn.linear_model' has no attribute 'GDRegressor'"
     ]
    }
   ],
   "source": [
    "clf = linear_model.GDRegressor(max_iter=num_iters, alpha= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
