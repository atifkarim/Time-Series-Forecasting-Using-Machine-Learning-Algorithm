{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_y(x):\n",
    "    y = x**2 - 4*x + 2\n",
    "    return y\n",
    "def gradient_descent(previous_x, learning_rate, epoch):\n",
    "    \n",
    "    # To fill with values\n",
    "    x_gd = []\n",
    "    y_gd = []\n",
    "    \n",
    "    x_gd.append(previous_x)\n",
    "    y_gd.append(func_y(previous_x))\n",
    "    # begin the loops to update x and y with out cost function\n",
    "    for i in range(epoch):\n",
    "        current_x = previous_x - learning_rate * (2*previous_x - 4)\n",
    "        x_gd.append(current_x)\n",
    "        y_gd.append(func_y(current_x))\n",
    "        # update previous_x\n",
    "        previous_x = current_x\n",
    "    return x_gd, y_gd\n",
    "# Initialize x0 and learning rate\n",
    "x0 = 4 # Our first 'guess' at what theta could be\n",
    "learning_rate = 0.15 # Alpha\n",
    "epoch = 100 # Number of tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = gradient_descent(x0, learning_rate, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3.4, 2.98, 2.686, 2.4802, 2.33614, 2.235298, 2.1647086, 2.11529602, 2.0807072140000002, 2.0564950498, 2.03954653486, 2.027682574402, 2.0193778020814, 2.01356446145698, 2.009495123019886, 2.0066465861139204, 2.004652610279744, 2.003256827195821, 2.0022797790370745, 2.0015958453259524, 2.0011170917281667, 2.0007819642097164, 2.0005473749468017, 2.0003831624627613, 2.000268213723933, 2.000187749606753, 2.000131424724727, 2.000091997307309, 2.0000643981151165, 2.0000450786805817, 2.000031555076407, 2.000022088553485, 2.0000154619874393, 2.0000108233912073, 2.000007576373845, 2.0000053034616916, 2.000003712423184, 2.000002598696229, 2.0000018190873603, 2.000001273361152, 2.0000008913528067, 2.0000006239469648, 2.000000436762875, 2.0000003057340128, 2.000000214013809, 2.000000149809666, 2.0000001048667664, 2.0000000734067367, 2.0000000513847156, 2.000000035969301, 2.000000025178511, 2.0000000176249575, 2.00000001233747, 2.000000008636229, 2.00000000604536, 2.0000000042317523, 2.0000000029622265, 2.0000000020735587, 2.000000001451491, 2.0000000010160437, 2.0000000007112306, 2.0000000004978613, 2.000000000348503, 2.000000000243952, 2.0000000001707665, 2.0000000001195364, 2.0000000000836753, 2.0000000000585727, 2.000000000041001, 2.0000000000287006, 2.0000000000200906, 2.0000000000140634, 2.0000000000098446, 2.0000000000068914, 2.000000000004824, 2.000000000003377, 2.000000000002364, 2.0000000000016547, 2.000000000001158, 2.000000000000811, 2.0000000000005675, 2.0000000000003975, 2.000000000000278, 2.0000000000001945, 2.0000000000001363, 2.0000000000000955, 2.0000000000000666, 2.0000000000000466, 2.000000000000033, 2.000000000000023, 2.000000000000016, 2.000000000000011, 2.000000000000008, 2.0000000000000058, 2.000000000000004, 2.0000000000000027, 2.0000000000000018, 2.0000000000000013, 2.000000000000001, 2.0000000000000004]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -0.040000000000000924, -1.0396, -1.5294040000000004, -1.7694079599999997, -1.8870099003999998, -1.9446348511959997, -1.9728710770860403, -1.9867068277721591, -1.9934863456083578, -1.9968083093480953, -1.9984360715805671, -1.9992336750744775, -1.999624500786494, -1.999816005385382, -1.9999098426388375, -1.99995582289303, -1.9999783532175845, -1.9999893930766168, -1.9999948026075423, -1.9999974532776958, -1.9999987521060705, -1.9999993885319745, -1.999999700380668, -1.9999998531865275, -1.9999999280613983, -1.999999964750085, -1.9999999827275419, -1.9999999915364954, -1.9999999958528827, -1.9999999979679126, -1.9999999990042774, -1.999999999512096, -1.9999999997609272, -1.9999999998828546, -1.9999999999425988, -1.9999999999718732, -1.9999999999862181, -1.9999999999932472, -1.9999999999966906, -1.9999999999983782, -1.999999999999205, -1.999999999999611, -1.999999999999809, -1.9999999999999067, -1.9999999999999538, -1.9999999999999778, -1.9999999999999893, -1.9999999999999947, -1.9999999999999973, -1.9999999999999991, -1.9999999999999991, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the idea taken from https://towardsdatascience.com/implementation-of-multi-variate-linear-regression-in-python-using-gradient-descent-optimization-b02f386425b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[1,2,3,4],[8,5,2,5],[6,3,9,7]]\n",
    "A = np.array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [8, 5, 2, 5],\n",
       "       [6, 3, 9, 7]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "X_train = A[:,:-1]\n",
    "y_train = A[:,-1]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.15\n",
    "num_iters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, alpha, num_iters):\n",
    "    n = X.shape[1] # n is number of feature\n",
    "    print('number of feature: ',n)\n",
    "    one_column = np.ones((X.shape[0],1))\n",
    "    print('one column :', one_column)\n",
    "    print('X is: ',X)\n",
    "    X = np.concatenate((one_column, X), axis = 1)\n",
    "    print('now X: ',X)\n",
    "    # initializing the parameter vector...\n",
    "    theta = np.zeros(n+1)\n",
    "    print('theta: ', theta)\n",
    "    # hypothesis calculation....\n",
    "    h = hypothesis(theta, X, n)\n",
    "    # returning the optimized parameters by Gradient Descent...\n",
    "    theta, cost = BGD(theta,alpha,num_iters,h,X,y,n)\n",
    "    return theta, cost\n",
    "\n",
    "def BGD(theta, alpha, num_iters, h, X, y, n):\n",
    "    print('coming input in BGD: theta',theta,'\\n h: ',h,'\\n y: ',y,'\\n X shape: ', X.shape)\n",
    "    print('after getting now h here BGD func will work')\n",
    "    cost = np.ones(num_iters)\n",
    "    print('cost: ', cost)\n",
    "    for i in range(0,num_iters):\n",
    "        print('---------here h and y : ', h,'\\t',y)\n",
    "        print('sum h-y: ', sum(h-y))\n",
    "        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)\n",
    "        print('theta o in BGD: ', theta[0])\n",
    "        for j in range(1,n+1):\n",
    "            print('j: ',j, ' ,X transpose: ', X.transpose()[j])\n",
    "            print('alpha/X.shape[0]',(alpha/X.shape[0]))\n",
    "            print('sum((h-y) * X.transpose()[j]', sum((h-y) * X.transpose()[j]))\n",
    "            theta[j] = theta[j] - (alpha/X.shape[0]) * sum((h-y) * X.transpose()[j])\n",
    "            print('theta j in BGD: ', theta[j])\n",
    "        h = hypothesis(theta, X, n)\n",
    "        cost[i] = (1/X.shape[0]) * 0.5 * sum(np.square(h - y))\n",
    "        print('i: ',i,' ,COST -------', cost[i])\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    return theta, cost\n",
    "\n",
    "def hypothesis(theta, X, n):\n",
    "    print('coming theta: ', theta)\n",
    "    print('\\n now hypothesis function')\n",
    "    h = np.ones((X.shape[0],1))\n",
    "    print('h: ',h)\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    print('reshaped theta', theta)\n",
    "    for i in range(0,X.shape[0]):\n",
    "        h[i] = float(np.matmul(theta, X[i]))\n",
    "    h = h.reshape(X.shape[0])\n",
    "    print('now h: ',h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of feature:  3\n",
      "one column : [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "X is:  [[1 2 3]\n",
      " [8 5 2]\n",
      " [6 3 9]]\n",
      "now X:  [[1. 1. 2. 3.]\n",
      " [1. 8. 5. 2.]\n",
      " [1. 6. 3. 9.]]\n",
      "theta:  [0. 0. 0. 0.]\n",
      "coming theta:  [0. 0. 0. 0.]\n",
      "\n",
      " now hypothesis function\n",
      "h:  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "reshaped theta [[0. 0. 0. 0.]]\n",
      "now h:  [0. 0. 0.]\n",
      "coming input in BGD: theta [0. 0. 0. 0.] \n",
      " h:  [0. 0. 0.] \n",
      " y:  [4 5 7] \n",
      " X shape:  (3, 4)\n",
      "after getting now h here BGD func will work\n",
      "cost:  [1. 1. 1.]\n",
      "---------here h and y :  [0. 0. 0.] \t [4 5 7]\n",
      "sum h-y:  -16.0\n",
      "theta o in BGD:  0.7999999999999999\n",
      "j:  1  ,X transpose:  [1. 8. 6.]\n",
      "alpha/X.shape[0] 0.049999999999999996\n",
      "sum((h-y) * X.transpose()[j] -86.0\n",
      "theta j in BGD:  4.3\n",
      "j:  2  ,X transpose:  [2. 5. 3.]\n",
      "alpha/X.shape[0] 0.049999999999999996\n",
      "sum((h-y) * X.transpose()[j] -54.0\n",
      "theta j in BGD:  2.6999999999999997\n",
      "j:  3  ,X transpose:  [3. 2. 9.]\n",
      "alpha/X.shape[0] 0.049999999999999996\n",
      "sum((h-y) * X.transpose()[j] -85.0\n",
      "theta j in BGD:  4.25\n",
      "coming theta:  [0.8  4.3  2.7  4.25]\n",
      "\n",
      " now hypothesis function\n",
      "h:  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "reshaped theta [[0.8  4.3  2.7  4.25]]\n",
      "now h:  [23.25 57.2  72.95]\n",
      "i:  0  ,COST ------- 1240.8008333333332\n",
      "---------here h and y :  [23.25 57.2  72.95] \t [4 5 7]\n",
      "sum h-y:  137.39999999999998\n",
      "theta o in BGD:  -6.0699999999999985\n",
      "j:  1  ,X transpose:  [1. 8. 6.]\n",
      "alpha/X.shape[0] 0.049999999999999996\n",
      "sum((h-y) * X.transpose()[j] 832.55\n",
      "theta j in BGD:  -37.3275\n",
      "j:  2  ,X transpose:  [2. 5. 3.]\n",
      "alpha/X.shape[0] 0.049999999999999996\n",
      "sum((h-y) * X.transpose()[j] 497.35\n",
      "theta j in BGD:  -22.1675\n",
      "j:  3  ,X transpose:  [3. 2. 9.]\n",
      "alpha/X.shape[0] 0.049999999999999996\n",
      "sum((h-y) * X.transpose()[j] 755.7\n",
      "theta j in BGD:  -33.535\n",
      "coming theta:  [ -6.07   -37.3275 -22.1675 -33.535 ]\n",
      "\n",
      " now hypothesis function\n",
      "h:  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "reshaped theta [[ -6.07   -37.3275 -22.1675 -33.535 ]]\n",
      "now h:  [-188.3375 -482.5975 -598.3525]\n",
      "i:  1  ,COST ------- 106866.11419479165\n",
      "---------here h and y :  [-188.3375 -482.5975 -598.3525] \t [4 5 7]\n",
      "sum h-y:  -1285.2875\n",
      "theta o in BGD:  58.19437499999999\n",
      "j:  1  ,X transpose:  [1. 8. 6.]\n",
      "alpha/X.shape[0] 0.049999999999999996\n",
      "sum((h-y) * X.transpose()[j] -7725.2325\n",
      "theta j in BGD:  348.934125\n",
      "j:  2  ,X transpose:  [2. 5. 3.]\n",
      "alpha/X.shape[0] 0.049999999999999996\n",
      "sum((h-y) * X.transpose()[j] -4638.719999999999\n",
      "theta j in BGD:  209.76849999999996\n",
      "j:  3  ,X transpose:  [3. 2. 9.]\n",
      "alpha/X.shape[0] 0.049999999999999996\n",
      "sum((h-y) * X.transpose()[j] -7000.379999999999\n",
      "theta j in BGD:  316.4839999999999\n",
      "coming theta:  [ 58.194375 348.934125 209.7685   316.484   ]\n",
      "\n",
      " now hypothesis function\n",
      "h:  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "reshaped theta [[ 58.194375 348.934125 209.7685   316.484   ]]\n",
      "now h:  [1776.1175   4531.477875 5629.460625]\n",
      "i:  2  ,COST ------- 9206910.977724358\n"
     ]
    }
   ],
   "source": [
    "theta, cost = linear_regression(X_train, y_train,learning_rate, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.44113812e+08 -2.07007978e+09 -1.24246965e+09 -1.87533872e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.24080083e+03 1.06866114e+05 9.20691098e+06 7.93211625e+08\n",
      " 6.83383058e+10 5.88761421e+12 5.07241155e+14 4.37008234e+16\n",
      " 3.76499806e+18 3.24369413e+20]\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDRegressor(max_iter=num_iters, alpha= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.15, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=10, n_iter=None, penalty='l2',\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: [0.27884363 0.32251253 0.48547914]\n",
      "Intercept: [0.14142455]\n"
     ]
    }
   ],
   "source": [
    "print('Slope:' ,clf.coef_)\n",
    "print('Intercept:', clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.linear_model' has no attribute 'GDRegressor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-74262e1ddacf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGDRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn.linear_model' has no attribute 'GDRegressor'"
     ]
    }
   ],
   "source": [
    "clf = linear_model.GDRegressor(max_iter=num_iters, alpha= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
